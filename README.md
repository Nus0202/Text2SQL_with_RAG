# ğŸ§  Text2SQL with Retrieval-Augmented Generation (RAG)

A modular, locally-deployable **Text-to-SQL** system integrating **Retrieval-Augmented Generation (RAG)** with **Large Language Models (LLMs)** to convert natural language questions into executable SQL queriesâ€”without fine-tuning.

> **Authors:** Zhengxiao Sun
> **Course:** CIS6400 â€” Machine Learning for Sequences  
> **Institution:** University of Guelph, Canada  
> **Date:** July 2025

---

## ğŸš€ Project Overview

This project explores how combining Retrieval-Augmented Generation (RAG) with LLMs can improve **zero-shot** Text-to-SQL performance. The system transforms user questions into SQL queries grounded in relevant schema segmentsâ€”retrieved dynamically using vector embeddingsâ€”thereby avoiding hallucinations and improving accuracy for complex SQL tasks.

---

## ğŸ§© Key Features

- ğŸ§¾ **Zero-shot SQL generation** without fine-tuning
- ğŸ” **Semantic retrieval** using FAISS and SentenceTransformers
- ğŸ“¦ **Local inference** with Deepseek-8B model via Ollama
- ğŸ”§ **Modular pipeline**: preprocessing â†’ embedding â†’ retrieval â†’ prompt â†’ generation
- ğŸ“Š **Manual evaluation** on 100 curated Spider-based examples

---

## ğŸ—ï¸ System Architecture

### ğŸ”„ Pipeline Steps

1. **User Query Input** â€“ Accept natural language query.
2. **Schema Embedding & Indexing** â€“ Encode schema using `MiniLM` and index with FAISS.
3. **Top-k Schema Retrieval** â€“ Retrieve most relevant schema chunks.
4. **Prompt Construction** â€“ Combine schema and query into structured prompt.
5. **LLM Inference** â€“ Generate SQL using `Deepseek-r1:8B` locally.
6. **Evaluation** â€“ Manual comparison with ground-truth SQL for correctness.

### ğŸ“Œ System Design Highlights

- **No cloud APIs** required: fully local and privacy-friendly.
- **Easy to extend**: plug-and-play with other LLMs or embedding models.
- **Runtime efficiency**: fast, low-resource design with modularity.

---

## ğŸ“Š Evaluation Results

| Model Setup      | Identical (Green) | Equivalent (Yellow) | Incorrect (Red) | Total Accuracy |
|------------------|------------------|----------------------|------------------|----------------|
| **w/o RAG**      | 8                | 8                    | 84               | **16%**        |
| **with RAG**     | 16               | 56                   | 28               | **72%**        |

âœ”ï¸ RAG significantly boosts both syntactic and semantic correctness  
âœ”ï¸ Particularly strong in **multi-table joins** and **nested subqueries**

---

## âš ï¸ Known Limitations

- ğŸ’¬ LLM may generate explanatory text along with SQL
- ğŸ”£ Occasional formatting/syntax inconsistencies (e.g., aliasing issues)
- ğŸ•’ Struggles with temporal expressions like date ranges
- ğŸ’¡ Lacks automatic syntax validation/post-processing

---

## ğŸ”® Future Work

- âœ… Add a lightweight SQL validator and formatter
- âœ‚ï¸ Remove extra commentary in output using stricter prompts
- ğŸ§  Improve alias consistency and temporal parsing
- ğŸ§ª Incorporate semantic post-checking (e.g., query execution validation)
- ğŸ§¬ Experiment with better embeddings and retrievers

---

## ğŸ“ File Structure

```bash
.
â”œâ”€â”€ src/                    # Python modules for each pipeline stage
â”‚   â”œâ”€â”€ preprocess.py       # Schema parsing
â”‚   â”œâ”€â”€ embed_index.py      # FAISS index builder
â”‚   â”œâ”€â”€ retriever.py        # Vector similarity search
â”‚   â”œâ”€â”€ prompt_builder.py   # Prompt formatting
â”‚   â”œâ”€â”€ generate_sql.py     # LLM inference using Ollama
â”‚   â””â”€â”€ evaluate.py         # Evaluation scripts
â”œâ”€â”€ data/                   # Processed schema and test queries
â”œâ”€â”€ results/                # Evaluation outputs and logs
â”œâ”€â”€ report/                 # Final project report (PDF)
â””â”€â”€ README.md
